{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f3adb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from feature_engine import imputation as mdi\n",
    "from feature_engine import encoding as ce\n",
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "from feature_engine.discretisation import EqualWidthDiscretiser\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "import mean_median2 as mm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def join_abs_path(p1, p2):\n",
    "    return os.path.abspath(os.path.join(p1, p2))\n",
    "\n",
    "\n",
    "def position_Y_COL(cols):  # Y label을 멘뒤로 위치\n",
    "    cols = cols.copy()\n",
    "    cols.remove(Y_COL)\n",
    "    return cols + [Y_COL]\n",
    "\n",
    "\n",
    "def read_data(afile):    \n",
    "    if config_dict['date_col'] is np.nan:\n",
    "        df = pd.read_csv(afile, usecols=config_dict['keep_col'])\n",
    "    else:\n",
    "        if config_dict['date_col'] in config_dict['keep_col']:\n",
    "            df = pd.read_csv(afile, usecols=config_dict['keep_col'], parse_dates=config_dict['date_col'])\n",
    "        else:\n",
    "            df = pd.read_csv(afile, usecols=config_dict['keep_col'])\n",
    "    \n",
    "    cols = list(df.columns)\n",
    "    cols = position_Y_COL(cols)\n",
    "    return df[cols]  \n",
    "\n",
    "\n",
    "def y_label_enc(df):\n",
    "    df = df.copy()\n",
    "    if df[Y_COL].isnull().any():\n",
    "        Y_null_exist = True\n",
    "    else:\n",
    "        Y_null_exist = False\n",
    "    labeler = LabelEncoder()\n",
    "    df[Y_COL] = labeler.fit_transform(df[Y_COL])\n",
    "    return df, Y_null_exist\n",
    "\n",
    "\n",
    "def discrete_cont(df):\n",
    "    data = df.copy()\n",
    "    # numerical: discrete vs continuous\n",
    "    if (config_dict['date_col'] is np.nan):\n",
    "        date_cols_len = 0\n",
    "    else:\n",
    "        date_cols_len = len(config_dict['date_col'])\n",
    "    if date_cols_len < 1:\n",
    "        discrete = [var for var in data.columns if\n",
    "                    data[var].dtype != 'O' and var != Y_COL and data[var].nunique() < config_dict['discrete_thresh_hold']]\n",
    "        continuous = [var for var in data.columns if\n",
    "                      data[var].dtype != 'O' and var != Y_COL and var not in discrete]\n",
    "    else:\n",
    "        discrete = [var for var in data.columns if\n",
    "                    data[var].dtype != 'O' and var != Y_COL and var not in config_dict['date_col'] and data[var].nunique() < config_dict['discrete_thresh_hold']]\n",
    "        continuous = [var for var in data.columns if\n",
    "                      data[var].dtype != 'O' and var != Y_COL and var not in config_dict['date_col'] and var not in discrete]\n",
    "    \n",
    "    # categorical\n",
    "    categorical = [var for var in data.columns if data[var].dtype == 'O' and var != Y_COL]\n",
    "\n",
    "    print('There are {} date_time variables'.format(date_cols_len))\n",
    "    print('There are {} discrete variables'.format(len(discrete)))\n",
    "    print('There are {} continuous variables'.format(len(continuous)))\n",
    "    print('There are {} categorical variables'.format(len(categorical)))\n",
    "    return discrete, continuous, categorical\n",
    "\n",
    "\n",
    "def separate_mixed(df):\n",
    "    df = df.copy()    \n",
    "    s = config_dict['mixed_str'][0]\n",
    "    e = config_dict['mixed_str'][1]\n",
    "    mixed_col = config_dict['mixed'][0]\n",
    "    df[mixed_col+'num'] = df[mixed_col].str.extract('(\\d+)') # captures numerical part\n",
    "    df[mixed_col+'num'] = df[mixed_col+'num'].astype('float')\n",
    "    df[mixed_col+'cat'] = df[mixed_col].str[s:e] # captures the first letter\n",
    "\n",
    "    # drop original mixed\n",
    "    df.drop([mixed_col], axis=1, inplace=True)\n",
    "    cols = position_Y_COL(list(df.columns))\n",
    "    return df[cols]\n",
    "\n",
    "\n",
    "def discretiser(df, numeric):\n",
    "    df = df.copy()\n",
    "    method = config_dict['discretiser'][0]\n",
    "    col = config_dict['discretiser'][1]\n",
    "    if method == 'equalwidth':\n",
    "        trans = EqualWidthDiscretiser()\n",
    "        X = df[[col]]\n",
    "        trans.fit(X)\n",
    "        df[col] = trans.transform(X)[col]\n",
    "    elif method == 'equalfrequency':\n",
    "        trans = EqualFrequencyDiscretiser()\n",
    "        X = df[[col]]\n",
    "        trans.fit(X)\n",
    "        df[col] = trans.transform(X)[col]\n",
    "    else:\n",
    "        print('Method Not Available')\n",
    "    return df\n",
    "\n",
    "\n",
    "def ohe(df):\n",
    "    df = df.copy()   \n",
    "    cols = config_dict['ohe']\n",
    "    for col in cols:\n",
    "        trans = OneHotEncoder()\n",
    "        X = df[[col]]\n",
    "        trans.fit(X)\n",
    "        df[col] = trans.transform(X)[col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def find_boundaries(df, variable, distance):\n",
    "    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n",
    "    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n",
    "    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n",
    "    return upper_boundary, lower_boundary\n",
    "\n",
    "\n",
    "def outlier(df):\n",
    "    df = df.copy()\n",
    "    cols = config_dict['outlier']\n",
    "    for c in cols:\n",
    "        upper_limit, lower_limit = find_boundaries(df, c, config_dict['iqr'])\n",
    "        outliers_ = np.where(df[c] > upper_limit, True,\n",
    "                    np.where(df[c] < lower_limit, True, False))\n",
    "        df = df.loc[~(outliers_)]\n",
    "    return df    \n",
    "\n",
    "\n",
    "def organize_data(df, y_null_exist):\n",
    "    df = df.copy()\n",
    "    cols = list(df.columns)\n",
    "    cols.remove(Y_COL)\n",
    "    null_threshhold_cols = []\n",
    "    discrete, continuous, categorical = discrete_cont(df)\n",
    "\n",
    "    for col in cols:\n",
    "        null_mean = df[col].isnull().mean()\n",
    "        if null_mean >= config_dict['null_threshhold']:\n",
    "            null_threshhold_cols.append(col)\n",
    "\n",
    "    cols_stayed = [c for c in cols if c not in null_threshhold_cols]\n",
    "    df = df[cols_stayed+[Y_COL]].copy()\n",
    "\n",
    "    if y_null_exist:\n",
    "        df = df[df[Y_COL] != df[Y_COL].max()].copy()\n",
    "\n",
    "    return df, discrete, continuous, categorical\n",
    "\n",
    "\n",
    "def make_train_test(df):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=Y_COL)\n",
    "    y = df[Y_COL]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=config_dict['test_size'], random_state=0, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def make_imputer_pipe(continuous, discrete, categorical, null_impute_type):\n",
    "    numberImputer = continuous + discrete\n",
    "    categoricalImputer = categorical.copy()\n",
    "    categoricalImputer = [item for item in categoricalImputer if item not in config_dict['ohe']]\n",
    "    oheImputer = config_dict['ohe']\n",
    "\n",
    "    if (len(numberImputer) > 0) & (len(categoricalImputer) > 0):\n",
    "        pipe = Pipeline([\n",
    "            (\"imputer\",\n",
    "             mm.MeanMedianImputer2(\n",
    "                 imputation_method=null_impute_type, variables=numberImputer),),\n",
    "            ('imputer_cat',\n",
    "             mdi.CategoricalImputer(variables=categorical)),\n",
    "            ('categorical_encoder',\n",
    "             ce.OneHotEncoder(variables=oheImputer)),\n",
    "            ('categorical_encoder2',\n",
    "             ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                               variables=categoricalImputer))\n",
    "        ])\n",
    "    else:\n",
    "        if (len(numberImputer) > 0) & (len(categoricalImputer) == 0):\n",
    "            pipe = Pipeline([\n",
    "                (\"imputer\",\n",
    "                 mm.MeanMedianImputer2(\n",
    "                     imputation_method=null_impute_type, variables=numberImputer),)\n",
    "            ])\n",
    "        else:\n",
    "            if (len(numberImputer) == 0) & (len(categoricalImputer) > 0):\n",
    "                pipe = Pipeline([\n",
    "                    ('imputer_cat',\n",
    "                     mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "                    ('categorical_encoder',\n",
    "                     ce.OneHotEncoder(variables=oheImputer)),\n",
    "                    ('categorical_encoder2',\n",
    "                     ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                                       variables=categoricalImputer))\n",
    "                ])\n",
    "            else:\n",
    "                pipe = []\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def do_imputation(df, pipe):\n",
    "    xtrain, xtest, y_train, y_test = make_train_test(df)\n",
    "\n",
    "    # pipe.fit(X_train, y_train)\n",
    "    pipe.fit(xtrain, y_train)\n",
    "    X_train = pipe.transform(xtrain)\n",
    "    X_test = pipe.transform(xtest)\n",
    "\n",
    "    X_train[Y_COL] = y_train        \n",
    "    X_train['split'] = 'train'\n",
    "    X_test[Y_COL] = y_test\n",
    "    X_test['split'] = 'test'        \n",
    "    return pd.concat([X_train, X_test]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def scaling(df):    \n",
    "    df = df.copy()\n",
    "    if config_dict['scale'] is np.nan:\n",
    "        config_dict['scale'] =='minmax'   # default with minmax scaling\n",
    "    if config_dict['scale'] =='minmax':\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df)\n",
    "        return scaler.transform(df)\n",
    "    elif config_dict['scale'] =='standard':\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df)\n",
    "        return scaler.transform(df)\n",
    "    else: \n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(df)\n",
    "        return scaler.transform(df)        \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # arv 예1: credit \n",
    "    # arv 예2: metro \n",
    "\n",
    "    try:\n",
    "        parent = join_abs_path(os.getcwd(), os.pardir)\n",
    "        folder = sys.argv[1]    # take input with argv parameter\n",
    "        conf_file = f'argumet_{folder}.xlsx'      \n",
    "        configs = pd.read_excel(join_abs_path(f'{parent}/config', conf_file), header=None).set_index(0)        \n",
    "        config_cols = configs.index.tolist()\n",
    "        config_dict = {}\n",
    "        for c in config_cols:\n",
    "            config_dict[c] = configs.loc[c].values[0]\n",
    "            if (type(config_dict[c]) == int) or (type(config_dict[c]) == float):\n",
    "                pass\n",
    "            else:\n",
    "                config_dict[c] = configs.loc[c].values[0].split(',')\n",
    "        ori_file_name = config_dict['file_name'][0].split('.')[0]\n",
    "        if config_dict['mixed_str'] is np.nan or len(config_dict['mixed_str']) < 1:\n",
    "            pass\n",
    "        else:\n",
    "            config_dict['mixed_str'] = [eval(i) for i in config_dict['mixed_str']]\n",
    "\n",
    "        if config_dict['y_col'] is np.nan or len(config_dict['y_col']) != 1:\n",
    "            print('No Y column exists')\n",
    "            raise Exception\n",
    "\n",
    "        if config_dict['discrete_thresh_hold'] is np.nan or config_dict['discrete_thresh_hold'] < 0:\n",
    "            print('discrete_thresh_hold set to default 10')\n",
    "            config_dict['discrete_thresh_hold'] = 10\n",
    "\n",
    "        Y_COL = config_dict['y_col'][0]\n",
    "        original_file = join_abs_path(f'{parent}/data/{folder}', config_dict['file_name'][0])\n",
    "        df_initial = read_data(original_file)\n",
    "\n",
    "        # 1. Label column Encoding\n",
    "        df_labeld, y_null_exist = y_label_enc(df_initial)\n",
    "\n",
    "        # 2. discrete, continuous, categorical 구분작업\n",
    "        df_organized, discrete, continuous, categorical = organize_data(df_labeld, y_null_exist)\n",
    "\n",
    "        # 3. Mixed 칼럼을 숫자형/문자형으로 분리\n",
    "        if config_dict['mixed'] is not np.nan:\n",
    "            df = separate_mixed(df_organized)\n",
    "            discrete, continuous, categorical = discrete_cont(df)\n",
    "        else:\n",
    "            df = df_organized.copy()\n",
    "\n",
    "        # null_impute_types 정의\n",
    "        null_impute_types = config_dict['null_imp']\n",
    "\n",
    "        if null_impute_types is not np.nan:\n",
    "            for null_impute_type in null_impute_types:\n",
    "        # 4. pipeline 정의\n",
    "                pipe = make_imputer_pipe(discrete, continuous, categorical, null_impute_type)\n",
    "\n",
    "                if pipe == []:\n",
    "                    print('no pipe applied')\n",
    "                else:\n",
    "        # 5. imputation thru pipeline\n",
    "                    df_piped = do_imputation(df, pipe)\n",
    "                    dest_path = os.path.join(parent, os.path.join('data_preprocessed', f'{folder}'))\n",
    "                    dest_path = os.path.join(parent, os.path.join(dest_path, f'{dest_path}/imputed'))\n",
    "                    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "                    dest_path = os.path.join(parent, os.path.join(dest_path, f'imputed_{ori_file_name}_{null_impute_type}.csv'))\n",
    "        # 5.1 imputation 저장\n",
    "                    df_piped.to_csv(dest_path, index=False)\n",
    "\n",
    "        # 6. discretization\n",
    "                    if config_dict['discretiser'] is not np.nan:    \n",
    "                        df_piped = discretiser(df_piped, discrete+continuous)\n",
    "\n",
    "        # 7. Outlier 처리\n",
    "                    if config_dict['outlier'] is not np.nan:    \n",
    "                        df_piped = outlier(df_piped)\n",
    "                        df_piped = df_piped.reset_index(drop=True)\n",
    "\n",
    "        # 8. 스케일링 작업 및 저장/ Train과 Test 를 따로 스케일링\n",
    "        # 8.1 X_train 스케일링\n",
    "                    con = df_piped['split'] == 'train'\n",
    "                    X_train_scaled = scaling(df_piped[con].drop(columns=[Y_COL,'split']))\n",
    "                    X_train_scaled = pd.DataFrame(X_train_scaled)\n",
    "                    X_train_scaled[Y_COL] = df_piped[con][Y_COL]\n",
    "                    X_train_scaled['split'] = df_piped[con]['split']\n",
    "                    X_train_scaled.columns = df_piped.columns\n",
    "         # 8.2 X_test 스케일링\n",
    "                    con = df_piped['split'] == 'test'\n",
    "                    X_test_scaled = scaling(df_piped[con].drop(columns=[Y_COL,'split']))\n",
    "                    X_test_scaled = pd.DataFrame(X_test_scaled)\n",
    "                    tmp = df_piped.copy().reset_index()\n",
    "                    X_test_scaled['index'] = tmp[con]['index'].values\n",
    "                    X_test_scaled = X_test_scaled.set_index('index')\n",
    "                    X_test_scaled[Y_COL] = df_piped[con][Y_COL]\n",
    "                    X_test_scaled['split'] = df_piped[con]['split']\n",
    "                    X_test_scaled.columns = df_piped.columns\n",
    "                    X_test_scaled.index.name = None\n",
    "                    del tmp\n",
    "         # 8.3 data frame merge\n",
    "                    df_scaled = pd.concat([X_train_scaled, X_test_scaled])\n",
    "         # 8.4 scaling 저장\n",
    "                    dest_path = os.path.join(parent, os.path.join('data_preprocessed', f'{folder}'))\n",
    "                    dest_path = os.path.join(parent, os.path.join(dest_path, 'scaled'))\n",
    "                    Path(dest_path).mkdir(parents=True, exist_ok=True)\n",
    "                    dest_path = os.path.join(parent, os.path.join(dest_path, f'scaled_{ori_file_name}_{null_impute_type}.csv'))\n",
    "                    df_scaled.to_csv(dest_path, index=False)\n",
    "        print('Completed.')\n",
    "    except Exception as e:\n",
    "        exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "        print('비정상종료', e)\n",
    "        print(exc_type, exc_tb.tb_lineno)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
