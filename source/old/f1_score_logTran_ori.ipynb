{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad177881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from feature_engine import imputation as mdi\n",
    "from feature_engine import encoding as ce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_path():\n",
    "    cur_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(cur_path)\n",
    "    return cur_path, parent_path\n",
    "\n",
    "\n",
    "def file_path(data_path, file):\n",
    "    return os.path.abspath(os.path.join(data_path, f'{file}'))\n",
    "\n",
    "\n",
    "def df_write(data_path, df, file):\n",
    "    df = df.copy()\n",
    "    df.to_csv(os.path.abspath(os.path.join(data_path, file)), index=False)\n",
    "\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=configs['y_col'][0])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def model_selection(option='logic'):\n",
    "    if option == 'light':\n",
    "        return lgb.LGBMClassifier(random_state=0)\n",
    "    else:\n",
    "        return LogisticRegression(random_state=0)\n",
    "\n",
    "\n",
    "def read_data(configs):\n",
    "    if configs['date_col'][0] == ' ':\n",
    "        df = pd.read_csv(configs['file_name'][0])\n",
    "    else:\n",
    "        df = pd.read_csv(configs['file_name'][0], parse_dates=configs['date_col'])\n",
    "\n",
    "    if configs['remove_col'][0] == ' ':\n",
    "        pass\n",
    "    else:\n",
    "        if configs['remove_col'][0] in df.columns.to_list():\n",
    "            df = df.drop(configs['remove_col'][0], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def y_label_enc(df, configs):\n",
    "    df = df.copy()\n",
    "    Y_col = configs['y_col'][0]\n",
    "    if df[Y_col].isnull().any():\n",
    "        Y_null = True\n",
    "    else:\n",
    "        Y_null = False\n",
    "    labeler = LabelEncoder()\n",
    "    df[Y_col] = labeler.fit_transform(df[Y_col])\n",
    "    return df, Y_null\n",
    "\n",
    "\n",
    "def organize_data(df, configs, y_null):\n",
    "    df = df.copy()\n",
    "    cols = df.columns.to_list()\n",
    "    null_threshhold_cols = []\n",
    "    no_null_cols = []\n",
    "    date_time = configs['date_col']\n",
    "    Y_col = configs['y_col'][0]\n",
    "\n",
    "    for col in cols:\n",
    "        null_mean = df[col].isnull().mean()\n",
    "        if null_mean >= configs['null_threshhold'][0]:\n",
    "            null_threshhold_cols.append(col)\n",
    "        if null_mean == 0:\n",
    "            no_null_cols.append(col)\n",
    "\n",
    "    cols_stayed = [item for item in cols if item not in null_threshhold_cols]\n",
    "    data = df[cols_stayed].copy()\n",
    "\n",
    "    # numerical: discrete vs continuous\n",
    "    discrete = [var for var in cols_stayed if\n",
    "                data[var].dtype != 'O' and var != Y_col and var not in date_time and data[var].nunique() < 10]\n",
    "    continuous = [var for var in cols_stayed if\n",
    "                  data[var].dtype != 'O' and var != Y_col and var not in date_time and var not in discrete]\n",
    "\n",
    "    # categorical\n",
    "    categorical = [var for var in cols_stayed if data[var].dtype == 'O' and var != Y_col]\n",
    "\n",
    "    print('There are {} date_time variables'.format(len(date_time)))\n",
    "    print('There are {} discrete variables'.format(len(discrete)))\n",
    "    print('There are {} continuous variables'.format(len(continuous)))\n",
    "    print('There are {} categorical variables'.format(len(categorical)))\n",
    "\n",
    "    if y_null:\n",
    "        data = data[data[Y_col] != data[Y_col].max()].copy()\n",
    "    else:\n",
    "        data = data.copy()\n",
    "\n",
    "    return data, discrete, continuous, categorical\n",
    "\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=configs['test_size'][0], random_state=0, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def make_imputer_pipe(continuous, discrete, categorical):\n",
    "    numberImputer = continuous + discrete\n",
    "    categoricalImputer = categorical\n",
    "\n",
    "    if (len(numberImputer) > 0) & (len(categoricalImputer) > 0):\n",
    "        pipe = Pipeline([\n",
    "            (\"median_imputer\",\n",
    "             mdi.MeanMedianImputer(\n",
    "                 imputation_method=\"median\", variables=numberImputer),),\n",
    "\n",
    "            ('imputer_cat',\n",
    "             mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "            ('categorical_encoder',\n",
    "             ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                               variables=categoricalImputer))\n",
    "        ])\n",
    "    else:\n",
    "        if (len(numberImputer) > 0) & (len(categoricalImputer) == 0):\n",
    "            pipe = Pipeline([\n",
    "                (\"median_imputer\",\n",
    "                 mdi.MeanMedianImputer(\n",
    "                     imputation_method=\"median\", variables=numberImputer),)\n",
    "            ])\n",
    "        else:\n",
    "            if (len(numberImputer) == 0) & (len(categoricalImputer) > 0):\n",
    "                pipe = Pipeline([\n",
    "                    ('imputer_cat',\n",
    "                     mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "                    ('categorical_encoder',\n",
    "                     ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                                       variables=categoricalImputer))\n",
    "                ])\n",
    "            else:\n",
    "                pipe = []\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def do_imputation(X_train, X_test, y_train, y_test, pipe):\n",
    "    X_train, X_test, y_train, y_test = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "    if pipe != []:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        X_train = pipe.transform(X_train)\n",
    "        X_test = pipe.transform(X_test)\n",
    "    else:\n",
    "        print('no pipe applied')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def do_train(X_train, X_test, y_train, y_test, option):\n",
    "    X_train, X_test, y_train, y_test = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "    model = model_selection(option)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, option)\n",
    "\n",
    "\n",
    "def min_max_scale(df):\n",
    "    df = df.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    return scaler.transform(df)\n",
    "\n",
    "\n",
    "def metrics(y_test, pred, option):\n",
    "    y_test = y_test.copy()\n",
    "    pred = pred.copy()\n",
    "    accuracy = round(accuracy_score(y_test, pred), 2)\n",
    "    precision = round(precision_score(y_test, pred), 2)\n",
    "    recall = round(recall_score(y_test, pred), 2)\n",
    "    f1 = round(f1_score(y_test, pred), 2)\n",
    "    print(option, \"f1 점수:\", f1, \"정확도:\", accuracy, \"정밀도:\", precision, \"재현율:\", recall)\n",
    "    print(confusion_matrix(y_test, pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d30e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 date_time variables\n",
      "There are 0 discrete variables\n",
      "There are 29 continuous variables\n",
      "There are 0 categorical variables\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # arv 예1: credit argumet_credit.xlsx\n",
    "    # arv 예2: metro argumet_metro.xlsx\n",
    "\n",
    "# folder_name = sys.argv[1]\n",
    "# config_file_name = sys.argv[2]\n",
    "\n",
    "folder_name = 'credit'\n",
    "config_file_name = 'argumet_credit.xlsx'\n",
    "\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(cur_path, os.pardir))\n",
    "config_file = os.path.join(parent, os.path.join('config', f'{config_file_name}'))\n",
    "configs = pd.read_excel(config_file, header=None).set_index(0).T\n",
    "configs = configs.to_dict('list')\n",
    "ori_file_name = configs['file_name'][0]\n",
    "configs['file_name'][0] = os.path.join(parent, os.path.join('data', configs['file_name'][0]))\n",
    "df_initial = read_data(configs)\n",
    "\n",
    "df, y_null = y_label_enc(df_initial, configs)\n",
    "df_organized, discrete, continuous, categorical = organize_data(df, configs, y_null)\n",
    "X_train, X_test, y_train, y_test = split_train_test(df_organized, configs)\n",
    "pipe = make_imputer_pipe(discrete, continuous, categorical)\n",
    "X_train, X_test, y_train, y_test = do_imputation(X_train, X_test, y_train, y_test, pipe)\n",
    "\n",
    "# X_train_scaled = min_max_scale(X_train)\n",
    "# X_test_scaled = min_max_scale(X_test)\n",
    "# xtrains = pd.DataFrame(data=X_train_scaled, columns=X_train.columns)\n",
    "# xtests = pd.DataFrame(data=X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2a7cc5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': ['C:\\\\Users\\\\jh\\\\0py_dev\\\\digitalship\\\\data\\\\creditcard.csv'],\n",
       " 'y_col': ['Class'],\n",
       " 'date_col': [' '],\n",
       " 'remove_col': ['Time'],\n",
       " 'null_threshhold': [0.3],\n",
       " 'fold': [1.5],\n",
       " 'test_size': [0.3],\n",
       " 'drop': [False]}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa7798ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Class', 'V17', -0.3264810676503034, 0.3264810676503034]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "corr_mat = df_organized.corr(method='pearson')\n",
    "upper_corr_mat = corr_mat.where(\n",
    "    np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "  \n",
    "# Convert to 1-D series and drop Null values\n",
    "unique_corr_pairs = upper_corr_mat.unstack().dropna()\n",
    "  \n",
    "# Sort correlation pairs\n",
    "sorted_mat = unique_corr_pairs.sort_values()\n",
    "df_corr = pd.DataFrame(sorted_mat).reset_index()\n",
    "df_corr = df_corr[df_corr['level_0']=='Class']\n",
    "df_corr[1] = abs(df_corr[0])\n",
    "df_corr = df_corr.sort_values(by=[1], ascending=False)\n",
    "# df_corr\n",
    "list(df_corr[df_corr['level_0']=='Class'].iloc[:1].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4639c168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic f1 점수: 0.71 정확도: 1.0 정밀도: 0.87 재현율: 0.59\n",
      "[[85282    13]\n",
      " [   60    88]]\n",
      "light f1 점수: 0.06 정확도: 0.97 정밀도: 0.03 재현율: 0.65\n",
      "[[82460  2835]\n",
      " [   52    96]]\n"
     ]
    }
   ],
   "source": [
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    # fraud에 해당하는 column 데이터만 추출, 1/4 분위와 3/4 분위 지점을 np.percentile로 구함. \n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    # IQR을 구하고, IQR에 1.5를 곱하여 최대값과 최소값 지점 구함. \n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    # 최대값 보다 크거나, 최소값 보다 작은 값을 아웃라이어로 설정하고 DataFrame index 반환. \n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index\n",
    "def get_preprocessed_df(df):\n",
    "    df_copy = df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Amount'], axis=1, inplace=True)\n",
    "    # 이상치 데이터 삭제하는 로직 추가\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(cur_path, os.pardir))\n",
    "config_file = os.path.join(parent, os.path.join('config', f'{config_file_name}'))\n",
    "configs = pd.read_excel(config_file, header=None).set_index(0).T\n",
    "configs = configs.to_dict('list')\n",
    "ori_file_name = configs['file_name'][0]\n",
    "configs['file_name'][0] = os.path.join(parent, os.path.join('data', configs['file_name'][0]))\n",
    "df_initial = read_data(configs)\n",
    "df_initial = df_initial.drop(0)\n",
    "df_initial\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test(df_initial, configs)\n",
    "do_train(X_train, X_test, y_train, y_test,'logic')\n",
    "do_train(X_train, X_test, y_train, y_test,'light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15bd68b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed=get_preprocessed_df(df_initial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f596a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic f1 점수: 0.76 정확도: 1.0 정밀도: 0.88 재현율: 0.67\n",
      "[[85281    14]\n",
      " [   48    98]]\n",
      "light f1 점수: 0.32 정확도: 1.0 정밀도: 0.22 재현율: 0.55\n",
      "[[85014   281]\n",
      " [   66    80]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test(df_processed, configs)\n",
    "do_train(X_train, X_test, y_train, y_test,'logic')\n",
    "do_train(X_train, X_test, y_train, y_test,'light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9e7b896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic f1 점수: 0.76 정확도: 1.0 정밀도: 0.88 재현율: 0.67\n",
      "[[85281    14]\n",
      " [   48    98]]\n",
      "light f1 점수: 0.32 정확도: 1.0 정밀도: 0.22 재현율: 0.55\n",
      "[[85014   281]\n",
      " [   66    80]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = split_train_test(df_processed, configs)\n",
    "do_train(X_train, X_test, y_train, y_test,'logic')\n",
    "do_train(X_train, X_test, y_train, y_test,'light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e231060d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d9710",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f22d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708ee2e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17fe9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_path():\n",
    "    cur_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(cur_path)\n",
    "    return cur_path, parent_path\n",
    "\n",
    "def file_path(data_path, file):\n",
    "    return os.path.abspath(os.path.join(data_path, f'{file}'))\n",
    "\n",
    "def df_write(data_path, df, file):\n",
    "    df = df.copy()\n",
    "    df.to_csv(os.path.abspath(os.path.join(data_path, file)), index=False)\n",
    "\n",
    "def write_processed(train, test, naming, drop):\n",
    "    _, parent_path = get_path()\n",
    "    df = train.copy()\n",
    "    del train\n",
    "    df[Y_col] = test.to_list()\n",
    "    del test\n",
    "    df.to_csv(f'{parent_path}/result_data/{naming}.csv', index=drop)\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=configs['y_col'][0])    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def model_selection(option='logic'):\n",
    "    if option == 'light':\n",
    "        return lgb.LGBMClassifier(random_state=0)\n",
    "    elif option == 'boost':\n",
    "        return GradientBoostingClassifier(random_state=0)\n",
    "    else:\n",
    "        return LogisticRegression(random_state=0)\n",
    "    \n",
    "def read_data(configs):\n",
    "    if configs['date_col'][0] ==' ':\n",
    "        df = pd.read_csv(configs['file_name'][0])\n",
    "    else:\n",
    "        df = pd.read_csv(configs['file_name'][0], parse_dates=configs['date_col'])\n",
    "\n",
    "    if configs['remove_col'][0] == ' ':\n",
    "        pass        \n",
    "    else:\n",
    "        if configs['remove_col'][0] in df.columns.to_list():\n",
    "            df = df.drop(configs['remove_col'][0], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def y_label_enc(df, configs):\n",
    "    df = df.copy()\n",
    "    Y_col = configs['y_col'][0]\n",
    "    if df[Y_col].isnull().any():\n",
    "        Y_null = True\n",
    "    else:\n",
    "        Y_null = False\n",
    "    labeler = LabelEncoder()\n",
    "    df[Y_col] = labeler.fit_transform(df[Y_col])\n",
    "    return df, Y_null\n",
    "\n",
    "\n",
    "def organize_data(df, configs, y_null):\n",
    "    df = df.copy()\n",
    "    cols = df.columns.to_list()\n",
    "    null_threshhold_cols = []\n",
    "    no_null_cols = []\n",
    "    date_time = configs['date_col']\n",
    "    Y_col = configs['y_col'][0]\n",
    "    \n",
    "#     if configs['remove_col'][0] == ' ':\n",
    "#         pass\n",
    "#     else:\n",
    "#         df = df.drop(columns=configs['remove_col'][0])\n",
    "    \n",
    "    for col in cols:\n",
    "        null_mean = df[col].isnull().mean()\n",
    "        if null_mean >= configs['null_threshhold'][0]:\n",
    "            null_threshhold_cols.append(col)\n",
    "        if null_mean == 0:\n",
    "            no_null_cols.append(col)\n",
    "\n",
    "    cols_stayed = [item for item in cols if item not in null_threshhold_cols]\n",
    "    data = df[cols_stayed].copy()\n",
    "\n",
    "    # numerical: discrete vs continuous\n",
    "    discrete = [var for var in cols_stayed if\n",
    "                data[var].dtype != 'O' and var != Y_col and var not in date_time and data[var].nunique() < 10]\n",
    "    continuous = [var for var in cols_stayed if\n",
    "                  data[var].dtype != 'O' and var != Y_col and var not in date_time and var not in discrete]\n",
    "\n",
    "    # categorical\n",
    "    categorical = [var for var in cols_stayed if data[var].dtype == 'O' and var != Y_col]\n",
    "\n",
    "    print('There are {} date_time variables'.format(len(date_time)))\n",
    "    print('There are {} discrete variables'.format(len(discrete)))\n",
    "    print('There are {} continuous variables'.format(len(continuous)))\n",
    "    print('There are {} categorical variables'.format(len(categorical)))\n",
    "\n",
    "    if y_null:\n",
    "        data = data[data[Y_col] != data[Y_col].max()].copy()\n",
    "    else:\n",
    "        data = data.copy()\n",
    "\n",
    "    return data, discrete, continuous, categorical\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=configs['test_size'][0], random_state=0, stratify=y)  \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def make_imputer_pipe(continuous, discrete, categorical):\n",
    "#     numberImputer = [item for item in continuous + discrete if item not in no_null_cols]\n",
    "#     categoricalImputer = [item for item in categorical if item not in no_null_cols]\n",
    "    numberImputer = continuous + discrete\n",
    "    categoricalImputer = categorical\n",
    "\n",
    "    if (len(numberImputer)>0) & (len(categoricalImputer)>0):\n",
    "        pipe = Pipeline([\n",
    "                (\"median_imputer\",\n",
    "                 mdi.MeanMedianImputer(\n",
    "                     imputation_method=\"median\", variables=numberImputer),),\n",
    "\n",
    "                ('imputer_cat',\n",
    "                 mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "                ('categorical_encoder',\n",
    "                 ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                                   variables=categoricalImputer))\n",
    "            ])\n",
    "    else:\n",
    "        if (len(numberImputer)>0) & (len(categoricalImputer)==0):\n",
    "            pipe = Pipeline([\n",
    "                (\"median_imputer\",\n",
    "                 mdi.MeanMedianImputer(\n",
    "                     imputation_method=\"median\", variables=numberImputer),)\n",
    "            ])\n",
    "        else:\n",
    "            if (len(numberImputer)==0) & (len(categoricalImputer)>0):\n",
    "                pipe = Pipeline([\n",
    "                    ('imputer_cat',\n",
    "                     mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "                    ('categorical_encoder',\n",
    "                     ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                                       variables=categoricalImputer))\n",
    "                ])\n",
    "            else:\n",
    "                pipe = []\n",
    "    return pipe\n",
    "\n",
    "def do_imputation(X_train, X_test, y_train, y_test, pipe):\n",
    "    X_train, X_test, y_train, y_test = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "    if pipe != []:        \n",
    "        pipe.fit(X_train, y_train)\n",
    "        X_train = pipe.transform(X_train)\n",
    "        X_test = pipe.transform(X_test)\n",
    "    else:\n",
    "        print ('no pipe')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def do_train(X_train, X_test, y_train, y_test, option):\n",
    "    X_train, X_test, y_train, y_test = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "    model = model_selection(option)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, option)\n",
    "    # write_processed(X_test_t, y_test, 'X_test_transform', drop)\n",
    "    # write_processed(X_train_t, y_train, 'X_train_transform', drop)\n",
    "                \n",
    "def min_max_scale(df):\n",
    "    df = df.copy()    \n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    return scaler.transform(df)\n",
    "    \n",
    "def metrics(y_test,pred, option):\n",
    "    y_test = y_test.copy()\n",
    "    pred = pred.copy()\n",
    "    accuracy = round(accuracy_score(y_test, pred),2)\n",
    "    precision = round(precision_score(y_test, pred),2)\n",
    "    recall = round(recall_score(y_test, pred),2)\n",
    "    f1 = round(f1_score(y_test, pred),2)\n",
    "    print(option, \"f1 점수:\", f1, \"정확도:\", accuracy, \"정밀도:\", precision, \"재현율:\", recall)\n",
    "    print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cce4b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"C:\\Users\\jh\\0py_dev\\digitalship\\data\\MetroPT3(AirCompressor).csv\")\n",
    "# df = df.append(df.iloc[0])\n",
    "# import numpy as np\n",
    "# df.iloc[-1,1] = np.nan\n",
    "# df.to_csv(r\"C:\\Users\\jh\\0py_dev\\digitalship\\data\\MetroPT3(AirCompressor)2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31ed27d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP2</th>\n",
       "      <th>TP3</th>\n",
       "      <th>H1</th>\n",
       "      <th>DV_pressure</th>\n",
       "      <th>Reservoirs</th>\n",
       "      <th>Oil_temperature</th>\n",
       "      <th>Motor_current</th>\n",
       "      <th>COMP</th>\n",
       "      <th>DV_eletric</th>\n",
       "      <th>Towers</th>\n",
       "      <th>MPG</th>\n",
       "      <th>LPS</th>\n",
       "      <th>Pressure_switch</th>\n",
       "      <th>Oil_level</th>\n",
       "      <th>Caudal_impulses</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.358</td>\n",
       "      <td>9.340</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>9.358</td>\n",
       "      <td>53.600</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>9.348</td>\n",
       "      <td>9.332</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>9.348</td>\n",
       "      <td>53.675</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.338</td>\n",
       "      <td>9.322</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>9.338</td>\n",
       "      <td>53.600</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.328</td>\n",
       "      <td>9.312</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>9.328</td>\n",
       "      <td>53.425</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.318</td>\n",
       "      <td>9.302</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>9.318</td>\n",
       "      <td>53.475</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>9.382</td>\n",
       "      <td>9.370</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9.384</td>\n",
       "      <td>68.300</td>\n",
       "      <td>3.7425</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>9.366</td>\n",
       "      <td>9.356</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9.368</td>\n",
       "      <td>68.050</td>\n",
       "      <td>3.7625</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.356</td>\n",
       "      <td>9.346</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9.358</td>\n",
       "      <td>67.825</td>\n",
       "      <td>3.6550</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>-0.012</td>\n",
       "      <td>9.344</td>\n",
       "      <td>9.332</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9.344</td>\n",
       "      <td>67.650</td>\n",
       "      <td>3.7600</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>NaN</td>\n",
       "      <td>9.332</td>\n",
       "      <td>9.320</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>9.334</td>\n",
       "      <td>67.525</td>\n",
       "      <td>3.7350</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1048575 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           TP2    TP3     H1  DV_pressure  Reservoirs  Oil_temperature  \\\n",
       "0       -0.012  9.358  9.340       -0.024       9.358           53.600   \n",
       "1       -0.014  9.348  9.332       -0.022       9.348           53.675   \n",
       "2       -0.012  9.338  9.322       -0.022       9.338           53.600   \n",
       "3       -0.012  9.328  9.312       -0.022       9.328           53.425   \n",
       "4       -0.012  9.318  9.302       -0.022       9.318           53.475   \n",
       "...        ...    ...    ...          ...         ...              ...   \n",
       "1048570 -0.010  9.382  9.370       -0.018       9.384           68.300   \n",
       "1048571 -0.010  9.366  9.356       -0.018       9.368           68.050   \n",
       "1048572 -0.012  9.356  9.346       -0.018       9.358           67.825   \n",
       "1048573 -0.012  9.344  9.332       -0.018       9.344           67.650   \n",
       "1048574    NaN  9.332  9.320       -0.018       9.334           67.525   \n",
       "\n",
       "         Motor_current  COMP  DV_eletric  Towers  MPG  LPS  Pressure_switch  \\\n",
       "0               0.0400     1           0       1    1    0                1   \n",
       "1               0.0400     1           0       1    1    0                1   \n",
       "2               0.0425     1           0       1    1    0                1   \n",
       "3               0.0400     1           0       1    1    0                1   \n",
       "4               0.0400     1           0       1    1    0                1   \n",
       "...                ...   ...         ...     ...  ...  ...              ...   \n",
       "1048570         3.7425     1           0       1    1    0                1   \n",
       "1048571         3.7625     1           0       1    1    0                1   \n",
       "1048572         3.6550     1           0       1    1    0                1   \n",
       "1048573         3.7600     1           0       1    1    0                1   \n",
       "1048574         3.7350     1           0       1    1    0                1   \n",
       "\n",
       "         Oil_level  Caudal_impulses  y  \n",
       "0                1                1  0  \n",
       "1                1                1  0  \n",
       "2                1                1  0  \n",
       "3                1                1  0  \n",
       "4                1                1  0  \n",
       "...            ...              ... ..  \n",
       "1048570          1                1  0  \n",
       "1048571          1                1  0  \n",
       "1048572          1                1  0  \n",
       "1048573          1                1  0  \n",
       "1048574          1                1  0  \n",
       "\n",
       "[1048575 rows x 16 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_file_name = 'argumet_metro.xlsx'\n",
    "# config_file_name = sys.argv[1]\n",
    "cur_path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(cur_path, os.pardir))\n",
    "config_file = os.path.join(parent, os.path.join('config', f'{config_file_name}'))\n",
    "configs = pd.read_excel(config_file, header=None).set_index(0).T\n",
    "configs = configs.to_dict('list')\n",
    "configs['file_name'][0] = os.path.join(parent,os.path.join('data',configs['file_name'][0]))\n",
    "df = read_data(configs)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0be296a",
   "metadata": {},
   "source": [
    "#### initial score before pre_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "198b15a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m read_data(configs)\n\u001b[0;32m      2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m split_train_test(df, configs)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36mdo_train\u001b[1;34m(X_train, X_test, y_train, y_test, option)\u001b[0m\n\u001b[0;32m    165\u001b[0m model \u001b[38;5;241m=\u001b[39m model_selection(option)\n\u001b[0;32m    166\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m--> 167\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m metrics(y_test, y_pred, option)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    427\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    this class would be predicted.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:800\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    797\u001b[0m         )\n\u001b[0;32m    799\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 800\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    803\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:114\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    108\u001b[0m         allow_nan\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m np\u001b[38;5;241m.\u001b[39misinf(X)\u001b[38;5;241m.\u001b[39many()\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X)\u001b[38;5;241m.\u001b[39mall()\n\u001b[0;32m    112\u001b[0m     ):\n\u001b[0;32m    113\u001b[0m         type_err \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfinity\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_nan \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaN, infinity\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    115\u001b[0m             msg_err\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    116\u001b[0m                 type_err, msg_dtype \u001b[38;5;28;01mif\u001b[39;00m msg_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    117\u001b[0m             )\n\u001b[0;32m    118\u001b[0m         )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# for object dtype data, we only check for NaNs (GH-13254)\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "df = read_data(configs)\n",
    "X_train, X_test, y_train, y_test = split_train_test(df, configs)\n",
    "do_train(X_train, X_test, y_train, y_test,'logic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a16d29",
   "metadata": {},
   "source": [
    "#### second trail for f1 score after pre_processing with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edac1a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 date_time variables\n",
      "There are 8 discrete variables\n",
      "There are 7 continuous variables\n",
      "There are 0 categorical variables\n",
      "logic f1 점수: 0.62 정확도: 0.97 정밀도: 0.64 재현율: 0.6\n",
      "[[299708   3795]\n",
      " [  4426   6644]]\n",
      "light f1 점수: 0.87 정확도: 0.99 정밀도: 0.98 재현율: 0.78\n",
      "[[303293    210]\n",
      " [  2458   8612]]\n"
     ]
    }
   ],
   "source": [
    "df = read_data(configs)\n",
    "df, y_null = y_label_enc(df, configs)\n",
    "df, discrete, continuous, categorical = organize_data(df, configs, y_null)\n",
    "X_train, X_test, y_train, y_test = split_train_test(df, configs)\n",
    "pipe = make_imputer_pipe(discrete, continuous, categorical)\n",
    "X_train, X_test, y_train, y_test = do_imputation(X_train, X_test, y_train, y_test, pipe)\n",
    "do_train(X_train, X_test, y_train, y_test,'logic')\n",
    "do_train(X_train, X_test, y_train, y_test,'light')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589b4632",
   "metadata": {},
   "source": [
    "#### third trail for f1 score with extra pre_processing with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430b4c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 date_time variables\n",
      "There are 8 discrete variables\n",
      "There are 7 continuous variables\n",
      "There are 0 categorical variables\n",
      "logic f1 점수: 0.69 정확도: 0.98 정밀도: 0.62 재현율: 0.77\n",
      "[[298155   5348]\n",
      " [  2513   8557]]\n",
      "light f1 점수: 0.74 정확도: 0.98 정밀도: 0.93 재현율: 0.61\n",
      "[[302960    543]\n",
      " [  4295   6775]]\n"
     ]
    }
   ],
   "source": [
    "df = read_data(configs)\n",
    "df, y_null = y_label_enc(df, configs)\n",
    "df, discrete, continuous, categorical = organize_data(df, configs, y_null)\n",
    "X_train, X_test, y_train, y_test = split_train_test(df, configs)\n",
    "pipe = make_imputer_pipe(discrete, continuous, categorical)\n",
    "X_train, X_test, y_train, y_test = do_imputation(X_train, X_test, y_train, y_test, pipe)\n",
    "\n",
    "X_train_scaled = min_max_scale(X_train)\n",
    "X_test_scaled = min_max_scale(X_test)\n",
    "do_train(X_train_scaled, X_test_scaled, y_train, y_test,'logic')\n",
    "do_train(X_train_scaled, X_test_scaled, y_train, y_test,'light')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28214a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrains = pd.DataFrame(data = X_train_scaled, columns=X_train.columns)\n",
    "# xtests = pd.DataFrame(data = X_test_scaled, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e62492b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logic f1 점수: 0.69 정확도: 0.98 정밀도: 0.62 재현율: 0.77\n",
      "[[298155   5348]\n",
      " [  2513   8557]]\n"
     ]
    }
   ],
   "source": [
    "do_train(xtrains, xtests, y_train, y_test,'logic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f6e0d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
