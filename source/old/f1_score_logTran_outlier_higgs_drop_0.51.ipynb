{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad177881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from feature_engine import imputation as mdi\n",
    "from feature_engine import encoding as ce\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def get_path():\n",
    "    cur_path = os.getcwd()\n",
    "    parent_path = os.path.dirname(cur_path)\n",
    "    return cur_path, parent_path\n",
    "\n",
    "\n",
    "def file_path(data_path, file):\n",
    "    return os.path.abspath(os.path.join(data_path, f'{file}'))\n",
    "\n",
    "\n",
    "def df_write(data_path, df, file):\n",
    "    df = df.copy()\n",
    "    df.to_csv(os.path.abspath(os.path.join(data_path, file)), index=False)\n",
    "\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=configs['y_col'][0])\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def model_selection(option='logistic'):\n",
    "    if option == 'LGBM':\n",
    "        return lgb.LGBMClassifier(random_state=0)\n",
    "    elif option == 'logistic':\n",
    "        return LogisticRegression(random_state=0)\n",
    "    elif option == 'knn':\n",
    "        return KNeighborsClassifier(n_neighbors=3)\n",
    "    elif option =='cat':\n",
    "        return CatBoostClassifier(random_seed=0)        \n",
    "\n",
    "\n",
    "def read_data(configs):\n",
    "    if configs['date_col'][0] == ' ':\n",
    "        df = pd.read_csv(configs['file_name'][0])\n",
    "    else:\n",
    "        df = pd.read_csv(configs['file_name'][0], parse_dates=configs['date_col'])\n",
    "\n",
    "    if configs['remove_col'][0] == ' ':\n",
    "        pass\n",
    "    else:\n",
    "        if configs['remove_col'][0] in df.columns.to_list():\n",
    "            df = df.drop(configs['remove_col'][0], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def y_label_enc(df, configs):\n",
    "    df = df.copy()\n",
    "    Y_col = configs['y_col'][0]\n",
    "    if df[Y_col].isnull().any():\n",
    "        Y_null = True\n",
    "    else:\n",
    "        Y_null = False\n",
    "    labeler = LabelEncoder()\n",
    "    df[Y_col] = labeler.fit_transform(df[Y_col])\n",
    "    return df, Y_null\n",
    "\n",
    "\n",
    "def organize_data(df, configs, y_null):\n",
    "    df = df.copy()\n",
    "    cols = df.columns.to_list()\n",
    "    null_threshhold_cols = []\n",
    "    no_null_cols = []\n",
    "    date_time = configs['date_col']\n",
    "    Y_col = configs['y_col'][0]\n",
    "\n",
    "    for col in cols:\n",
    "        null_mean = df[col].isnull().mean()\n",
    "        if null_mean >= configs['null_threshhold'][0]:\n",
    "            null_threshhold_cols.append(col)\n",
    "        if null_mean == 0:\n",
    "            no_null_cols.append(col)\n",
    "\n",
    "    cols_stayed = [item for item in cols if item not in null_threshhold_cols]\n",
    "    data = df[cols_stayed].copy()\n",
    "\n",
    "    # numerical: discrete vs continuous\n",
    "    discrete = [var for var in cols_stayed if\n",
    "                data[var].dtype != 'O' and var != Y_col and var not in date_time and data[var].nunique() < 10]\n",
    "    continuous = [var for var in cols_stayed if\n",
    "                  data[var].dtype != 'O' and var != Y_col and var not in date_time and var not in discrete]\n",
    "\n",
    "    # categorical\n",
    "    categorical = [var for var in cols_stayed if data[var].dtype == 'O' and var != Y_col]\n",
    "\n",
    "    print('There are {} date_time variables'.format(len(date_time)))\n",
    "    print('There are {} discrete variables'.format(len(discrete)))\n",
    "    print('There are {} continuous variables'.format(len(continuous)))\n",
    "    print('There are {} categorical variables'.format(len(categorical)))\n",
    "\n",
    "    if y_null:\n",
    "        data = data[data[Y_col] != data[Y_col].max()].copy()\n",
    "    else:\n",
    "        data = data.copy()\n",
    "\n",
    "    return data, discrete, continuous, categorical\n",
    "\n",
    "\n",
    "def split_train_test(df, configs):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=configs['test_size'][0], random_state=0, stratify=y)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def make_imputer_pipe(continuous, discrete, categorical):\n",
    "    numberImputer = continuous + discrete\n",
    "    categoricalImputer = categorical\n",
    "\n",
    "    if (len(numberImputer) > 0) & (len(categoricalImputer) > 0):\n",
    "        pipe = Pipeline([\n",
    "            (\"median_imputer\",\n",
    "             mdi.MeanMedianImputer(\n",
    "                 imputation_method=\"median\", variables=numberImputer),),\n",
    "\n",
    "            ('imputer_cat',\n",
    "             mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "            ('categorical_encoder',\n",
    "             ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                               variables=categoricalImputer))\n",
    "        ])\n",
    "    else:\n",
    "        if (len(numberImputer) > 0) & (len(categoricalImputer) == 0):\n",
    "            pipe = Pipeline([\n",
    "                (\"median_imputer\",\n",
    "                 mdi.MeanMedianImputer(\n",
    "                     imputation_method=\"median\", variables=numberImputer),)\n",
    "            ])\n",
    "        else:\n",
    "            if (len(numberImputer) == 0) & (len(categoricalImputer) > 0):\n",
    "                pipe = Pipeline([\n",
    "                    ('imputer_cat',\n",
    "                     mdi.CategoricalImputer(variables=categoricalImputer)),\n",
    "\n",
    "                    ('categorical_encoder',\n",
    "                     ce.OrdinalEncoder(encoding_method='ordered',\n",
    "                                       variables=categoricalImputer))\n",
    "                ])\n",
    "            else:\n",
    "                pipe = []\n",
    "    return pipe\n",
    "\n",
    "\n",
    "def do_imputation(X_train, X_test, y_train, y_test, pipe):\n",
    "    X_train, X_test, y_train, y_test = X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy()\n",
    "    if pipe != []:\n",
    "        pipe.fit(X_train, y_train)\n",
    "        X_train = pipe.transform(X_train)\n",
    "        X_test = pipe.transform(X_test)\n",
    "    else:\n",
    "        print('no pipe applied')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def do_imputation_whole(df, pipe):\n",
    "    df = df.copy()\n",
    "    if pipe != []:\n",
    "        df = pipe.fit_transform(df)        \n",
    "    else:\n",
    "        print('no pipe applied')\n",
    "    return df\n",
    "\n",
    "\n",
    "def do_train(X_train, y_train, option):\n",
    "    X_train, y_train = X_train.copy(), y_train.copy()\n",
    "    model = model_selection(option)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def do_train_cat(X_train, y_train):\n",
    "    X_train, y_train = X_train.copy(), y_train.copy()\n",
    "    model = model_selection('cat')\n",
    "    model.fit(X_train, y_train, silent=True)\n",
    "    return model\n",
    "\n",
    "    \n",
    "def do_predict(model, X_test):\n",
    "    return model.predict(X_test)\n",
    "\n",
    "\n",
    "def min_max_scale(df):\n",
    "    df = df.copy()\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(df)\n",
    "    return scaler.transform(df)\n",
    "\n",
    "\n",
    "def metrics(file, y_test, pred, option, display_confusion=False, logtran=None, out_col=None):\n",
    "    y_test = y_test.copy()\n",
    "    pred = pred.copy()\n",
    "    accuracy = round(accuracy_score(y_test, pred), 3)\n",
    "    precision = round(precision_score(y_test, pred), 3)\n",
    "    recall = round(recall_score(y_test, pred), 3)\n",
    "    f1 = round(f1_score(y_test, pred), 3)\n",
    "#     print(file, option, \"f1 점수:\", f1, \"정확도:\", accuracy, \"정밀도:\", precision, \"재현율:\", recall)\n",
    "    if display_confusion:        \n",
    "        print(confusion_matrix(y_test, pred))\n",
    "    \n",
    "    scores = [file, option, logtran, out_col, f1, accuracy, precision, recall]\n",
    "    cols = ['File', 'Model', 'LogTrans', 'Outliered', 'F1', 'Accuracy', 'Precision', 'Recall']\n",
    "    return pd.DataFrame(data=[scores], columns=cols)\n",
    "\n",
    "\n",
    "def drop_outlier(df=None, corr_highest=None, y_col=None, yes_value=None, weight=1.5):\n",
    "    df = df.copy()\n",
    "    targeted = df[df[y_col]==yes_value][corr_highest]\n",
    "    quantile_25 = np.percentile(targeted.values, 25)\n",
    "    quantile_75 = np.percentile(targeted.values, 75)\n",
    "\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "\n",
    "    outlier_index = targeted [(targeted  < lowest_val) | (targeted > highest_val)].index\n",
    "    df = df.drop(outlier_index, axis=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def log_trans(df, trans_col):\n",
    "    df = df.copy()\n",
    "    trans_values = np.log1p(df[trans_col])\n",
    "    df.drop([trans_col], axis=1, inplace=True)\n",
    "    df.insert(0, trans_col, trans_values)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_skew_top3(df):\n",
    "    df = df.copy()\n",
    "    skewed = df.skew()\n",
    "    skewed = pd.DataFrame(data=skewed).reset_index()\n",
    "    idx = skewed[skewed['index']==Y_col].index\n",
    "    skewed = skewed.drop(idx, axis=0)\n",
    "    skewed = skewed.rename(columns={'index':'col', 0:'skewed_value'})\n",
    "    skewed['abs_skewed_value'] = abs(skewed['skewed_value'])\n",
    "    skewed = skewed.sort_values(by=['abs_skewed_value'], ascending=False).reset_index(drop=True)\n",
    "    skewed_higher = skewed.iloc[0:3]\n",
    "    return skewed_higher\n",
    "\n",
    "\n",
    "def get_corr_top5(df, y_col, yes_value):\n",
    "    df = df.copy()\n",
    "    corr_mat = df.corr(method='pearson')\n",
    "    upper_corr_mat = corr_mat.where(np.triu(np.ones(corr_mat.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Convert to 1-D series and drop Null values\n",
    "    unique_corr_pairs = upper_corr_mat.unstack().dropna()\n",
    "\n",
    "    # Sort correlation pairs\n",
    "    sorted_mat = unique_corr_pairs.sort_values()\n",
    "    df_corr = pd.DataFrame(data=sorted_mat).reset_index()\n",
    "    df_corr = df_corr[df_corr['level_0']==y_col]\n",
    "    df_corr = df_corr.rename(columns={'level_0':'Y_col', 'level_1':'col', 0:'corr_value'})\n",
    "    df_corr['abs_corr_value'] = abs(df_corr['corr_value'])\n",
    "    df_corr = df_corr.sort_values(by=['abs_corr_value'], ascending=False).reset_index(drop=True)\n",
    "    corr_higher = df_corr.iloc[0:3]\n",
    "    return corr_higher\n",
    "\n",
    "def split_impute_train2(file, df, configs, option='logic', logtran=None, out_col=None):\n",
    "    df = df.copy()\n",
    "    Y_col = configs['y_col'][0]\n",
    "    \n",
    "    con = df['split']=='train'\n",
    "    X_train = df[con].drop(columns=['split', Y_col]) \n",
    "    y_train = df[con][Y_col]\n",
    "    con = df['split']=='test'\n",
    "    X_test = df[con].drop(columns=['split', Y_col]) \n",
    "    y_test = df[con][Y_col]\n",
    "    \n",
    "    if option=='cat':\n",
    "        model = do_train_cat(X_train, y_train)\n",
    "    else:\n",
    "        model = do_train(X_train, y_train, option)\n",
    "    pred = do_predict(model, X_test)\n",
    "    result = metrics(file, y_test, pred, option, display_confusion=False, logtran=logtran, out_col=out_col)\n",
    "    return result\n",
    "    \n",
    "\n",
    "def split_impute_train(df, configs, discrete, continuous, categorical, option='logic', logtran=None, out_col=None):\n",
    "    X_train, X_test, y_train, y_test = split_train_test(df, configs)\n",
    "    pipe = make_imputer_pipe(discrete, continuous, categorical)\n",
    "    X_train, X_test, y_train, y_test = do_imputation(X_train, X_test, y_train, y_test, pipe)\n",
    "    model = do_train(X_train, y_train, option)\n",
    "    pred = do_predict(model, X_test)\n",
    "    result = metrics(y_test, pred, option, display_confusion=False, logtran=logtran, out_col=out_col)\n",
    "    return result\n",
    "\n",
    "def pca_train_metric(df, configs, option='logic', logtran=None, out_col=None):\n",
    "    df = df.copy()\n",
    "    X = df.drop(columns=configs['y_col'][0])\n",
    "    y = df[configs['y_col'][0]]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=configs['test_size'][0], random_state=0, stratify=y)\n",
    "    model = do_train(X_train, y_train, option)\n",
    "    pred = do_predict(model, X_test)\n",
    "    result = metrics(y_test, pred, option, display_confusion=False, logtran=logtran, out_col=out_col)\n",
    "    return result\n",
    "\n",
    "def pca_train_metric2(file, df, configs, option='logic', logtran=None, out_col=None):\n",
    "    df = df.copy()\n",
    "    Y_col = configs['y_col'][0]\n",
    "    \n",
    "    con = df['split']=='train'\n",
    "    X_train = df[con].drop(columns=['split', Y_col]) \n",
    "    y_train = df[con][Y_col]\n",
    "    con = df['split']=='test'\n",
    "    X_test = df[con].drop(columns=['split', Y_col]) \n",
    "    y_test = df[con][Y_col]    \n",
    "    model = do_train(X_train, y_train, option)\n",
    "    pred = do_predict(model, X_test)\n",
    "    result = metrics(file, y_test, pred, option, display_confusion=False, logtran=logtran, out_col=out_col)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d30e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = 'higgs'\n",
    "config_file_name = f'argumet_{folder_name}.xlsx'\n",
    "\n",
    "cur_path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(cur_path, os.pardir))\n",
    "config_file = os.path.join(parent, os.path.join('config', f'{config_file_name}'))\n",
    "configs = pd.read_excel(config_file, header=None).set_index(0).T\n",
    "configs = configs.to_dict('list')\n",
    "ori_file_name = configs['file_name'][0]\n",
    "configs['file_name'][0] = os.path.join(parent, os.path.join('data', configs['file_name'][0]))\n",
    "Y_col = configs['y_col'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b1941",
   "metadata": {},
   "source": [
    "### before log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46784f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'split'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Model</th>\n",
       "      <th>LogTrans</th>\n",
       "      <th>Outliered</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_max.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_mean.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_median.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_median.csv</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.749</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_min.csv</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.751</td>\n",
       "      <td>0.789</td>\n",
       "      <td>0.823</td>\n",
       "      <td>0.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_max.csv</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_mean.csv</td>\n",
       "      <td>LGBM</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.756</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_min.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File     Model LogTrans Outliered     F1  Accuracy  \\\n",
       "0     imputed_higgs_max.csv  logistic     None      None  0.523     0.611   \n",
       "0    imputed_higgs_mean.csv  logistic     None      None  0.613     0.709   \n",
       "0  imputed_higgs_median.csv  logistic     None      None  0.698     0.768   \n",
       "0  imputed_higgs_median.csv      LGBM     None      None  0.749     0.792   \n",
       "0     imputed_higgs_min.csv      LGBM     None      None  0.751     0.789   \n",
       "0     imputed_higgs_max.csv      LGBM     None      None  0.756     0.797   \n",
       "0    imputed_higgs_mean.csv      LGBM     None      None  0.756     0.797   \n",
       "0     imputed_higgs_min.csv  logistic     None      None  0.772     0.728   \n",
       "\n",
       "   Precision  Recall  \n",
       "0      0.598   0.465  \n",
       "0      0.786   0.503  \n",
       "0      0.868   0.584  \n",
       "0      0.841   0.675  \n",
       "0      0.823   0.690  \n",
       "0      0.843   0.685  \n",
       "0      0.841   0.687  \n",
       "0      0.628   1.000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/imputed')))\n",
    "df = {}\n",
    "for file in files:\n",
    "    df[file] = pd.read_csv( os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/imputed', f'{file}')))\n",
    "    \n",
    "results = pd.DataFrame()\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        result = split_impute_train2(file, df[file], configs, option='logistic', logtran=None, out_col=None)\n",
    "        results = results.append(result)\n",
    "        result = split_impute_train2(file, df[file], configs, option='LGBM', logtran=None, out_col=None)\n",
    "        results = results.append(result)\n",
    "    except Exception as e:    \n",
    "        print(e)\n",
    "    \n",
    "results.sort_values('F1', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99629380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>File</th>\n",
       "      <th>Model</th>\n",
       "      <th>LogTrans</th>\n",
       "      <th>Outliered</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_max.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.598</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scaled_higgs_max.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.602</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_mean.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.613</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scaled_higgs_mean.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.614</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_median.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.768</td>\n",
       "      <td>0.868</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scaled_higgs_median.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.758</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.611</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scaled_higgs_min.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.762</td>\n",
       "      <td>0.713</td>\n",
       "      <td>0.616</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>imputed_higgs_min.csv</td>\n",
       "      <td>logistic</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       File     Model LogTrans Outliered     F1  Accuracy  \\\n",
       "0     imputed_higgs_max.csv  logistic     None      None  0.523     0.611   \n",
       "0      scaled_higgs_max.csv  logistic     None      None  0.525     0.613   \n",
       "0    imputed_higgs_mean.csv  logistic     None      None  0.613     0.709   \n",
       "0     scaled_higgs_mean.csv  logistic     None      None  0.614     0.709   \n",
       "0  imputed_higgs_median.csv  logistic     None      None  0.698     0.768   \n",
       "0   scaled_higgs_median.csv  logistic     None      None  0.758     0.707   \n",
       "0      scaled_higgs_min.csv  logistic     None      None  0.762     0.713   \n",
       "0     imputed_higgs_min.csv  logistic     None      None  0.772     0.728   \n",
       "\n",
       "   Precision  Recall  \n",
       "0      0.598   0.465  \n",
       "0      0.602   0.465  \n",
       "0      0.786   0.503  \n",
       "0      0.787   0.503  \n",
       "0      0.868   0.584  \n",
       "0      0.611   1.000  \n",
       "0      0.616   1.000  \n",
       "0      0.628   1.000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = os.listdir(os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/scaled')))\n",
    "df = {}\n",
    "for file in files:\n",
    "    df[file] = pd.read_csv( os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/scaled', f'{file}')))\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        result = split_impute_train2(file, df[file], configs, option='logistic', logtran=None, out_col=None)\n",
    "        results = results.append(result)\n",
    "#         result = split_impute_train2(file, df[file], configs, option='LGBM', logtran=None, out_col=None)\n",
    "#         results = results.append(result)\n",
    "    except Exception as e:    \n",
    "        print(e)\n",
    "    \n",
    "results[results['Model']=='logistic'].sort_values('F1', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75938fcd",
   "metadata": {},
   "source": [
    "### after log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a42990f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#             print('Logtrans 칼럼', trans_col)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m             df_log_processed \u001b[38;5;241m=\u001b[39m log_trans(df[file], trans_col)\n\u001b[1;32m---> 14\u001b[0m             result \u001b[38;5;241m=\u001b[39m \u001b[43msplit_impute_train2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_log_processed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfigs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlogistic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogtran\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m             results \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#             result = split_impute_train2(file, df_log_processed, configs, option='LGBM', logtran=trans_col, out_col=None)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#             results = results.append(result)\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#             print()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m#         result = split_impute_train2(file, df_log_processed, configs, option='LGBM', logtran='combined', out_col=None)\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#         results = results.append(result)\u001b[39;00m\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36msplit_impute_train2\u001b[1;34m(file, df, configs, option, logtran, out_col)\u001b[0m\n\u001b[0;32m    295\u001b[0m     model \u001b[38;5;241m=\u001b[39m do_train_cat(X_train, y_train)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mdo_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m pred \u001b[38;5;241m=\u001b[39m do_predict(model, X_test)\n\u001b[0;32m    299\u001b[0m result \u001b[38;5;241m=\u001b[39m metrics(file, y_test, pred, option, display_confusion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, logtran\u001b[38;5;241m=\u001b[39mlogtran, out_col\u001b[38;5;241m=\u001b[39mout_col)\n",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36mdo_train\u001b[1;34m(X_train, y_train, option)\u001b[0m\n\u001b[0;32m    189\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcopy(), y_train\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    190\u001b[0m model \u001b[38;5;241m=\u001b[39m model_selection(option)\n\u001b[1;32m--> 191\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1589\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1587\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1588\u001b[0m     prefer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1589\u001b[0m fold_coefs_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1590\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1591\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1592\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_joblib_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1593\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1594\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1596\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mC_\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1599\u001b[0m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1603\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1606\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1612\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclasses_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarm_start_coef\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1617\u001b[0m fold_coefs_, _, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_iter_, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32)[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1086\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    820\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:806\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    803\u001b[0m     iprint \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m101\u001b[39m][\n\u001b[0;32m    804\u001b[0m         np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]), verbose)\n\u001b[0;32m    805\u001b[0m     ]\n\u001b[1;32m--> 806\u001b[0m     opt_res \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mw0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43miprint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43miprint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m     n_iter_i \u001b[38;5;241m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    815\u001b[0m         solver,\n\u001b[0;32m    816\u001b[0m         opt_res,\n\u001b[0;32m    817\u001b[0m         max_iter,\n\u001b[0;32m    818\u001b[0m         extra_warning_msg\u001b[38;5;241m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    819\u001b[0m     )\n\u001b[0;32m    820\u001b[0m     w0, loss \u001b[38;5;241m=\u001b[39m opt_res\u001b[38;5;241m.\u001b[39mx, opt_res\u001b[38;5;241m.\u001b[39mfun\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    353\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray_equal(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;124;03m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_if_needed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mall(x \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(x)\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjac \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m fg[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:124\u001b[0m, in \u001b[0;36m_logistic_loss_and_grad\u001b[1;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;66;03m# Logistic loss is the negative of the log of the logistic function.\u001b[39;00m\n\u001b[0;32m    122\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(sample_weight \u001b[38;5;241m*\u001b[39m log_logistic(yz)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m0.5\u001b[39m \u001b[38;5;241m*\u001b[39m alpha \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(w, w)\n\u001b[1;32m--> 124\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mexpit\u001b[49m\u001b[43m(\u001b[49m\u001b[43myz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m z0 \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m (z \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m y\n\u001b[0;32m    127\u001b[0m grad[:n_features] \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X\u001b[38;5;241m.\u001b[39mT, z0) \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m w\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "files = os.listdir(os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/scaled')))\n",
    "df = {}\n",
    "for file in files:\n",
    "#     df[file] = pd.read_csv( os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/scaled', f'{file}')))\n",
    "    df[file] = pd.read_csv( os.path.join(parent, os.path.join('data_preprocessed', f'{folder_name}/scaled', f'{file}')))\n",
    "\n",
    "try:\n",
    "    for file in files:\n",
    "        skewed = get_skew_top3(df[file])\n",
    "        for idx, row in skewed.iterrows():\n",
    "            trans_col = row[0]\n",
    "#             print('Logtrans 칼럼', trans_col)\n",
    "            df_log_processed = log_trans(df[file], trans_col)\n",
    "            result = split_impute_train2(file, df_log_processed, configs, option='logistic', logtran=trans_col, out_col=None)\n",
    "            results = results.append(result)\n",
    "#             result = split_impute_train2(file, df_log_processed, configs, option='LGBM', logtran=trans_col, out_col=None)\n",
    "#             results = results.append(result)\n",
    "#             print()\n",
    "\n",
    "#         df_log_processed = df[file].copy()\n",
    "#         for idx, row in skewed.iterrows():\n",
    "#             trans_col = row[0]\n",
    "#             df_log_processed = log_trans(df_log_processed, trans_col)\n",
    "#         result = split_impute_train2(file, df_log_processed, configs, option='logistic', logtran='combined', out_col=None)\n",
    "#         results = results.append(result)\n",
    "#         result = split_impute_train2(file, df_log_processed, configs, option='LGBM', logtran='combined', out_col=None)\n",
    "#         results = results.append(result)\n",
    "except Exception as e:    \n",
    "    print(e)\n",
    "results[results['Model']=='logistic'].sort_values('F1', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37873566",
   "metadata": {},
   "source": [
    "### after removing outlier with log transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed1e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if folder_name == 'credit':\n",
    "#     trans_col = 'Amount'\n",
    "# else:\n",
    "#     trans_col = 'DV_pressure'\n",
    "yes_value = 1\n",
    "\n",
    "for file in files:\n",
    "    corr_higher = get_corr_top5(df[file], Y_col, yes_value)\n",
    "\n",
    "    for idx_s, row_s in skewed.iterrows():\n",
    "        trans_col = row_s[0]    \n",
    "        for idx_c, row_c in corr_higher.iterrows():\n",
    "            print('Logtrans 칼럼', trans_col, 'Outlier 처리 칼럼', row_c['col'], row_c['corr_value'])\n",
    "            corr_highest = row_c[1]\n",
    "            df_processed = log_trans(df[file], trans_col)\n",
    "            df_processed = drop_outlier(df_processed, corr_highest, Y_col, yes_value, weight=1.5)\n",
    "            result = split_impute_train2(file, df_processed, configs, option='logistic', logtran=trans_col, out_col=corr_highest)           \n",
    "            results = results.append(result)\n",
    "#             result = split_impute_train2(file, df_processed, configs, option='LGBM', logtran=trans_col, out_col=corr_highest)           \n",
    "#             results = results.append(result)\n",
    "            print()\n",
    "    \n",
    "results[results['Model']=='logistic'].sort_values('F1', ascending=True)\n",
    "\n",
    "#     for idx_s, row_s in skewed.iterrows():\n",
    "#         trans_col = row_s[0]    \n",
    "#         for idx_c, row_c in corr_higher.iterrows():\n",
    "#             print('Logtrans 칼럼', trans_col, 'Outlier 처리 칼럼', row_c['col'], row_c['corr_value'])\n",
    "#             corr_highest = row_c[1]\n",
    "#             df_processed = log_trans(df[file], trans_col)\n",
    "#             df_processed = drop_outlier(df_processed, corr_highest, Y_col, yes_value, weight=1.5)\n",
    "#             result = split_impute_train2(file, df_processed, configs, option='LGBM', logtran=trans_col, out_col=corr_highest)\n",
    "#             results = results.append(result)\n",
    "#             print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae50b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use(['dark_background'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a58f67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ori = results.reset_index(drop=True)\n",
    "results_ori['cat'] = results_ori['LogTrans']+':'+results_ori['Outliered'] \n",
    "results_ori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = results_ori.sort_values(['F1','cat'], ascending=[True,False])#.set_index('cat')\n",
    "results_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f04e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted[results_sorted['Model']=='logistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b662bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d5937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
